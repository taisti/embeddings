{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2231142/2231142 [00:52<00:00, 42467.49it/s]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trening\n",
    "Import bibliotek i procedura normalizacyjna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import prodigy\n",
    "import spacy\n",
    "import csv\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "#from prodigy.models.ner import EntityRecognizer\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "#name_dict, desc_dict = load_food_entities()\n",
    "#kb = spacy.kb.KnowledgeBase(vocab=nlp.vocab, entity_vector_length=300)\n",
    "\n",
    "import re\n",
    "import itertools\n",
    "from itertools import chain, combinations\n",
    "import warnings\n",
    "warnings. filterwarnings('ignore')\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "ps = PorterStemmer()\n",
    "def normalize_name(name):\n",
    "    name = re.sub(r'[^a-zA-Z]', ' ', name)\n",
    "    name = re.sub(r'\\s+', ' ', name)\n",
    "    name = name.lower()\n",
    "    name = ' '.join([token for token in name.split(' ') if token not in stopwords])\n",
    "    stemmed_name = \" \".join([ps.stem(token.text) for token in nlp(name)])\n",
    "    return stemmed_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wczytanie zbioru danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"recipeNLG_with_entities.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ekstrakcja z JSON potrzebnych pól (wykryte NER o typie `FOOD`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2231142/2231142 [22:45:56<00:00, 27.22it/s]   \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "corpus = []\n",
    "json_fail_count = 0\n",
    "json_fail = []\n",
    "for ingredients_entities in tqdm(df[\"ingredients_entities\"].tolist()):\n",
    "    sentence = []\n",
    "    try:\n",
    "        for entity in json.loads(ingredients_entities):\n",
    "            if entity[\"type\"] == \"FOOD\":\n",
    "                sentence.append(\"_\".join(normalize_name(entity[\"entity\"]).split(\" \")))\n",
    "        corpus.append(\" \".join(sentence).lower())\n",
    "    except:\n",
    "        json_fail.append(ingredients_entities)\n",
    "        json_fail_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zapis do pliku wyesktrachowanej reprezentacji do treningu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset_new_forfasttext_lower.csv', 'w') as csvfile:\n",
    "    for i in corpus:\n",
    "        csvfile.write(i.lower())\n",
    "        csvfile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trening modelu, zapis wyniku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "model = fasttext.train_unsupervised('dataset_new_forfasttext_instructions.csv',thread=8, minn=4, maxn=6, dim=50, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"dataset_new_instructions.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32781"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wczytanie modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.9290194511413574, 'citi_chicken'),\n",
       " (0.893699049949646, 'mix_chicken_piec'),\n",
       " (0.8879286646842957, 'chicken_halv'),\n",
       " (0.8790009021759033, 'chicken_portion'),\n",
       " (0.8712169528007507, 'broil_chicken'),\n",
       " (0.8696103692054749, 'chicken_chicken'),\n",
       " (0.867918074131012, 'chicken_cut'),\n",
       " (0.8649100661277771, 'chicken_ala_king'),\n",
       " (0.8585206866264343, 'meat_chicken'),\n",
       " (0.8568282723426819, 'veggi_chicken')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fasttext\n",
    "model = fasttext.load_model(\"dataset_new.bin\")\n",
    "model.get_nearest_neighbors('chicken')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proste wyszukiwanie czy składniki znajdują się w modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gouda_chees',\n",
       " 'gouda',\n",
       " 'shred_gouda_chees',\n",
       " 'chees_gouda',\n",
       " 'goat_gouda',\n",
       " 'boar_s_head_chipotl_gouda']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in model.words if 'gouda' in i ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rekomendacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chicken\n",
      "(0.9290194511413574, 'citi_chicken')\n",
      "(0.893699049949646, 'mix_chicken_piec')\n",
      "(0.8879286646842957, 'chicken_halv')\n",
      "(0.8790009021759033, 'chicken_portion')\n",
      "(0.8712169528007507, 'broil_chicken')\n",
      "(0.8696103692054749, 'chicken_chicken')\n",
      "(0.867918074131012, 'chicken_cut')\n",
      "(0.8649100661277771, 'chicken_ala_king')\n",
      "(0.8585206866264343, 'meat_chicken')\n",
      "(0.8568282723426819, 'veggi_chicken')\n",
      "tofu\n",
      "(0.9260441660881042, 'tofu_silken')\n",
      "(0.8897832632064819, 'bake_tofu')\n",
      "(0.8827080726623535, 'egg_tofu')\n",
      "(0.8749420046806335, 'tofu_firm')\n",
      "(0.870351254940033, 'silk_tofu')\n",
      "(0.8676044940948486, 'silki_tofu')\n",
      "(0.8665119409561157, 'tofu_skin')\n",
      "(0.8615848422050476, 'silken_tofu_firm')\n",
      "(0.8584445118904114, 'silken_tofu')\n",
      "(0.8566809296607971, 'silken')\n",
      "milk\n",
      "(0.8302230834960938, 'evapor_milk')\n",
      "(0.8208866119384766, 'butter')\n",
      "(0.8154518008232117, 'egg')\n",
      "(0.8121494650840759, 'milnot_cream')\n",
      "(0.8046228885650635, 'milnot_milk')\n",
      "(0.803139865398407, 'dairi_half_half')\n",
      "(0.7996000647544861, 'evapor')\n",
      "(0.7978138327598572, 'carnat_evapor_milk')\n",
      "(0.79683518409729, 'margarin')\n",
      "(0.7900131344795227, 'pet_evapor_milk')\n",
      "egg\n",
      "(0.8154520392417908, 'milk')\n",
      "(0.8000625967979431, 'egg_unbeaten')\n",
      "(0.7926957011222839, 'unbeaten_egg')\n",
      "(0.7894937992095947, 'butter')\n",
      "(0.7841209769248962, 'egg_oor')\n",
      "(0.7738681435585022, 'salt')\n",
      "(0.7649752497673035, '</s>')\n",
      "(0.756428599357605, 'mik')\n",
      "(0.7557492256164551, 'margain')\n",
      "(0.7517643570899963, 'b_p')\n",
      "banana\n",
      "(0.902548611164093, 'dole_banana')\n",
      "(0.8916700482368469, 'bananna')\n",
      "(0.882179319858551, 'strawberri_banana_yogurt')\n",
      "(0.8785958290100098, 'med_banana')\n",
      "(0.8780544996261597, 'bannana')\n",
      "(0.8745425939559937, 'strawberri_banana')\n",
      "(0.8626872301101685, 'strawberri_banana_flavor')\n",
      "(0.8623504638671875, 'overrip_banana')\n",
      "(0.862296998500824, 'strawberri_banana_nectar')\n",
      "(0.8590580821037292, 'rip_banana')\n",
      "lentil\n",
      "(0.9081096053123474, 'split_lentil')\n",
      "(0.8812063336372375, 'masoor_lentil')\n",
      "(0.8740416765213013, 'urad_lentil')\n",
      "(0.8708998560905457, 'moong_lentil')\n",
      "(0.865321695804596, 'du_puy_lentil')\n",
      "(0.8652557730674744, 'mix_lentil')\n",
      "(0.8622457385063171, 'lentil_bean')\n",
      "(0.848703145980835, 'french_lentil')\n",
      "(0.8473439812660217, 'brown_lentil')\n",
      "(0.836576521396637, 'puy_lentil')\n",
      "soy_cream\n",
      "(0.8419979810714722, 'plain_soy_creamer')\n",
      "(0.8305408358573914, 'soy_creamer')\n",
      "(0.7835875153541565, 'soya_cream')\n",
      "(0.7811110615730286, 'soya_milk')\n",
      "(0.7789571285247803, 'soymilk')\n",
      "(0.7722225785255432, 'plain_soy_milk')\n",
      "(0.765313446521759, 'soy_cream_chees')\n",
      "(0.7645689249038696, 'soy_margarin')\n",
      "(0.7640460133552551, 'vegan_whip_cream')\n",
      "(0.7640421986579895, 'cashew_cream')\n",
      "butter\n",
      "(0.8651416897773743, 'margarin')\n",
      "(0.8280642032623291, 'magarin')\n",
      "(0.8208865523338318, 'milk')\n",
      "(0.8206565976142883, 'chiffon_margarin')\n",
      "(0.8184331059455872, 'oleo_margarin')\n",
      "(0.817577064037323, 'oleomargarin')\n",
      "(0.8059778213500977, 'margain')\n",
      "(0.8045403957366943, 'butter_margarin')\n",
      "(0.7969185709953308, 'margarin_butter')\n",
      "(0.7916005253791809, 'parkay_margarin')\n",
      "gouda\n",
      "(0.897088348865509, 'gouda_chees')\n",
      "(0.8351644277572632, 'emmental_chees')\n",
      "(0.8341633081436157, 'emment_chees')\n",
      "(0.8328235149383545, 'gruyer_chees')\n",
      "(0.8323578834533691, 'comt_chees')\n",
      "(0.8284788727760315, 'gruyr_chees')\n",
      "(0.8284211158752441, 'st_andr_chees')\n",
      "(0.8273360729217529, 'gruyer')\n",
      "(0.8250493407249451, 'mimolett_chees')\n",
      "(0.8190907835960388, 'emmenth_chees')\n"
     ]
    }
   ],
   "source": [
    "for test_words in ['chicken', 'tofu', 'milk', 'egg', 'banana', 'lentil',  'soy_cream', 'butter', 'gouda']:\n",
    "    print(test_words)\n",
    "    for i in model.get_nearest_neighbors(test_words):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eksport zanurzeń"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"embeddings.txt\", 'w') as f:\n",
    "    for i in model.get_words():\n",
    "        f.write(i)\n",
    "        \n",
    "        for j in model.get_word_vector(i):\n",
    "            f.write(\",\")\n",
    "            f.write(str(j))\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In a heavy 2-quart saucepan, mix brown sugar, nuts, evaporated milk and butter or margarine. Stir over medium heat until mixture bubbles all over top. Boil and stir 5 minutes more. Take off heat. Stir in vanilla and cereal; mix well. Using 2 teaspoons, drop and shape into 30 clusters on wax paper. Let stand until firm, about 30 minutes.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"directions\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dodatek: tworzenie pliku do treningu modelu na tekstach całych poleceń "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Założenie: zbiór danych został już wczytany tak jak w kodzie wyżej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 1809148/2231142 [00:03<00:01, 421758.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERR nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2231142/2231142 [00:04<00:00, 509885.51it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('dataset_new_forfasttext_instructions.csv', 'w') as csvfile:\n",
    "    for i in tqdm(df[\"directions\"]):\n",
    "        if type(i) !=  str:\n",
    "            print(\"ERR\",i)\n",
    "            continue\n",
    "        csvfile.write(i.lower())\n",
    "        csvfile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title                                                      Fancy Meatloaf\n",
       "ingredients             1/2 loaf Italian bread, crust removed, torn in...\n",
       "directions              Preheat the oven to 375 degrees. Soak the brea...\n",
       "link                                  cooking.nytimes.com/recipes/1012686\n",
       "source                                                          Recipes1M\n",
       "NER                     [\"Italian bread\", \"milk\", \"ground beef\", \"grou...\n",
       "ingredients_entities    [{\"start\": 0, \"end\": 3, \"type\": \"QUANTITY\", \"e...\n",
       "Name: 1809149, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[1809149,]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
